# UMLS Mapping Pipeline - Performance Optimization Analysis

**Ph√¢n t√≠ch chi ti·∫øt v√† gi·∫£i ph√°p t·ªëi ∆∞u h√≥a cho lu·ªìng d·ªØ li·ªáu l·ªõn**

---

## üìä 1. CURRENT PERFORMANCE ANALYSIS

### Benchmark - Current Runtime (Typical Dataset)

| Stage | Current Runtime | Bottleneck | Scalability |
|-------|----------------|------------|-------------|
| **Stage 0: UMLS Loading** | 30-45 min | Sequential I/O, CPU-bound parsing | ‚ö†Ô∏è Fixed per UMLS version |
| **Stage 1: Preprocessing** | 5-10 min | Acceptable | ‚úÖ Linear with entities |
| **Stage 2 Setup: SapBERT** | 2-3 hrs (GPU)<br>4-6 hrs (CPU) | üî¥ **CRITICAL** - Encoding 4M+ concepts | ‚ùå Non-scalable |
| **Stage 2 Setup: TF-IDF** | 10-15 min | Memory-intensive fit_transform | ‚ö†Ô∏è O(n*m) complexity |
| **Stage 2: Candidate Gen** | 15-30 min | Encoding queries, FAISS search | ‚ö†Ô∏è Linear with queries |
| **Stage 3: Aggregation** | 5-10 min | Acceptable | ‚úÖ Good |
| **Stage 4: Filtering** | 2-5 min | Acceptable | ‚úÖ Good |
| **Stage 5: Reranking** | 2-5 min | Lightweight | ‚úÖ Good |
| **Stage 6: Final Output** | 1-2 min | I/O bound | ‚úÖ Good |
| **TOTAL (first run)** | **3-5 hours** | üî¥ Stage 2 Setup dominates | |
| **TOTAL (cached)** | **30-60 min** | Stage 2 Candidate Gen | |

### Key Findings:

üî¥ **Critical Bottleneck:** Stage 2 Setup (SapBERT encoding)
- Encodes 4M+ UMLS concepts sequentially
- Each encoding: tokenize ‚Üí forward pass ‚Üí normalize
- GPU utilization often <50% due to small batch size

‚ö†Ô∏è **Secondary Bottlenecks:**
1. Stage 2 Candidate Generation: Query encoding (one-by-one)
2. Stage 0: UMLS file parsing (sequential, single-threaded)
3. TF-IDF: Memory-intensive matrix operations

‚úÖ **Acceptable Performance:** Stages 3-6

---

## üöÄ 2. OPTIMIZATION STRATEGIES

### Strategy 1: **Multi-GPU Parallelization** üèÜ

#### Current Implementation:
```python
# Single GPU, sequential processing
for cui in tqdm(cuis):
    emb = encode_text(preferred_name)  # One at a time
    umls_embeddings[cui] = emb
```

#### Optimized Implementation:
```python
# Multi-GPU with DataParallel
import torch.nn as nn

# Wrap model
model = nn.DataParallel(model, device_ids=[0, 1, 2, 3])

# Batch encoding
batch_size = 1024  # Large batch across GPUs
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    embeddings = encode_batch(batch)  # Parallel across GPUs
```

**Expected Speedup:**
- 4 GPUs: **4x faster** (2-3 hours ‚Üí 30-45 min)
- 8 GPUs: **7-8x faster** (2-3 hours ‚Üí 15-25 min)

**Cost:** Requires multiple GPUs (cloud: ~$2-5/hour for 4x V100)

---

### Strategy 2: **Larger Batch Sizes + Mixed Precision** üî•

#### Current:
```python
batch_size = 256  # Conservative
dtype = torch.float32  # Full precision
```

#### Optimized:
```python
batch_size = 2048  # 8x larger
dtype = torch.float16  # Half precision (AMP)

# Enable automatic mixed precision
from torch.cuda.amp import autocast

with autocast():
    embeddings = model(**inputs)
```

**Expected Speedup:**
- Batch size 2048: **2-3x faster**
- FP16 (mixed precision): **1.5-2x faster**
- **Combined: 3-6x faster** (2-3 hours ‚Üí 20-40 min)

**Memory:** FP16 uses 50% less GPU memory ‚Üí allows larger batches

---

### Strategy 3: **FAISS Approximate Indexing** ‚ö°

#### Current: Exact Search
```python
# IndexFlatIP: Exact but slow for queries
index = faiss.IndexFlatIP(dim)
index.add(vectors)  # Brute-force search: O(n*d)
```

#### Optimized: Approximate Search
```python
# IVF (Inverted File Index) with PQ (Product Quantization)
nlist = 4096  # Number of clusters
m = 64        # PQ subvectors
nbits = 8     # Bits per subvector

# Coarse quantizer
quantizer = faiss.IndexFlatIP(dim)

# IVF-PQ index
index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, nbits)

# Train index
index.train(vectors)
index.add(vectors)

# Search with nprobe
index.nprobe = 32  # Search in 32 clusters
```

**Expected Speedup:**
- Indexing: **10-100x faster** for large datasets
- Query search: **10-50x faster**
- Accuracy: 95-99% recall@128

**Trade-off:** Slight accuracy loss (<5%) for massive speed gain

---

### Strategy 4: **Parallel UMLS Parsing** üìÇ

#### Current: Sequential
```python
# Parse MRCONSO line by line
for line in tqdm(f):
    fields = line.split('|')
    # Process...
```

#### Optimized: Chunked Parallel
```python
import multiprocessing as mp
from functools import partial

def process_chunk(chunk_lines):
    """Process chunk of lines"""
    concepts = {}
    for line in chunk_lines:
        fields = line.split('|')
        # Process...
    return concepts

# Read file in chunks
chunk_size = 1_000_000
chunks = []

with open(mrconso_path) as f:
    chunk = []
    for line in f:
        chunk.append(line)
        if len(chunk) >= chunk_size:
            chunks.append(chunk)
            chunk = []

# Parallel processing
with mp.Pool(processes=16) as pool:
    results = pool.map(process_chunk, chunks)

# Merge results
final_concepts = {}
for chunk_result in results:
    final_concepts.update(chunk_result)
```

**Expected Speedup:**
- 16 cores: **10-15x faster** (30-45 min ‚Üí 2-3 min)

---

### Strategy 5: **Incremental/Streaming Processing** üåä

#### Current: Batch Processing
```python
# Load all entities at once
with open('entities.txt') as f:
    entities = [line.strip() for line in f]  # Load all

# Process all
for entity in entities:
    candidates = generate_candidates(entity)
```

#### Optimized: Streaming
```python
def entity_stream(file_path, chunk_size=1000):
    """Stream entities in chunks"""
    with open(file_path) as f:
        chunk = []
        for line in f:
            chunk.append(line.strip())
            if len(chunk) >= chunk_size:
                yield chunk
                chunk = []
        if chunk:
            yield chunk

# Process in streaming chunks
for entity_chunk in entity_stream('entities.txt'):
    # Batch encode chunk
    embeddings = encode_batch(entity_chunk)

    # Batch FAISS search
    scores, indices = index.search(embeddings, k=128)

    # Save chunk results
    save_chunk_results(entity_chunk, scores, indices)
```

**Benefits:**
- Constant memory usage (no matter dataset size)
- Can process unlimited entities
- Better GPU utilization (larger batches)

---

### Strategy 6: **Caching at Multiple Levels** üíæ

#### Current: Single-level Cache
```python
# Only cache final embeddings
cache_file = 'umls_embeddings.pkl'
```

#### Optimized: Multi-level Cache
```python
# Level 1: Tokenization cache
tokenization_cache = {}  # text ‚Üí token_ids

# Level 2: Embedding cache (by text hash)
embedding_cache = {}  # text_hash ‚Üí embedding

# Level 3: FAISS results cache
faiss_cache = {}  # query_hash ‚Üí (scores, indices)

# Level 4: Final candidates cache
candidates_cache = {}  # entity ‚Üí candidates
```

**Benefits:**
- Skip tokenization for repeated texts
- Reuse embeddings for identical queries
- Cache FAISS results for common queries
- Incremental updates (add new entities without recomputing)

---

### Strategy 7: **TF-IDF Optimization** üìä

#### Current: Dense Operation
```python
# Fit on all 10M+ aliases at once
tfidf_matrix = vectorizer.fit_transform(all_aliases)  # Memory-intensive
```

#### Optimized: Sparse + Incremental
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import vstack, save_npz, load_npz

# Incremental fit
vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(3, 3),
    max_features=100000,
    dtype=np.float32  # Use float32 instead of float64
)

# Process in chunks
chunk_size = 1_000_000
matrices = []

for i in range(0, len(all_aliases), chunk_size):
    chunk = all_aliases[i:i+chunk_size]

    if i == 0:
        vectorizer.fit(chunk)  # Fit vocabulary on first chunk

    chunk_matrix = vectorizer.transform(chunk)
    matrices.append(chunk_matrix)

# Stack sparse matrices
tfidf_matrix = vstack(matrices)

# Save as sparse
save_npz('tfidf_matrix_sparse.npz', tfidf_matrix)
```

**Benefits:**
- 50% memory reduction (float32 vs float64)
- Chunked processing avoids OOM
- Sparse storage ~10x smaller

---

### Strategy 8: **Model Quantization** üóúÔ∏è

#### Current: Full Precision
```python
model = AutoModel.from_pretrained(model_name)  # FP32
```

#### Optimized: Quantized Model
```python
# INT8 quantization
from transformers import AutoModelForSequenceClassification
import torch.quantization

model = AutoModel.from_pretrained(model_name)
model.eval()

# Dynamic quantization
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # Quantize linear layers
    dtype=torch.qint8
)
```

**Benefits:**
- 4x smaller model size
- 2-3x faster inference on CPU
- Same accuracy (minimal degradation)

---

### Strategy 9: **Distributed Computing** üåê

#### Setup: Ray for Distributed Processing
```python
import ray
from ray import remote

ray.init(num_cpus=32, num_gpus=4)

@remote(num_gpus=0.25)
class EmbeddingWorker:
    def __init__(self, model_name):
        self.model = AutoModel.from_pretrained(model_name)
        self.model.cuda()

    def encode_batch(self, texts):
        return encode_texts(texts, self.model)

# Create workers
workers = [EmbeddingWorker.remote(model_name) for _ in range(4)]

# Distribute work
futures = []
chunk_size = 100000
for i, worker in enumerate(workers):
    start = i * chunk_size
    end = (i + 1) * chunk_size
    future = worker.encode_batch.remote(texts[start:end])
    futures.append(future)

# Gather results
results = ray.get(futures)
```

**Benefits:**
- Scale to multiple machines
- Automatic load balancing
- Fault tolerance

---

## üìà 3. EXPECTED PERFORMANCE GAINS

### Optimization Roadmap

| Strategy | Difficulty | Speedup | Cost | Priority |
|----------|-----------|---------|------|----------|
| **Larger Batch + FP16** | Easy | 3-6x | Free | üî• HIGH |
| **FAISS IVF-PQ** | Medium | 10-50x | Free | üî• HIGH |
| **Parallel UMLS Parse** | Easy | 10-15x | Free | ‚úÖ HIGH |
| **Streaming Processing** | Medium | Memory‚àû | Free | ‚úÖ HIGH |
| **Multi-level Caching** | Easy | 2-10x* | Storage | ‚úÖ MEDIUM |
| **Multi-GPU** | Medium | 4-8x | $$$ GPUs | üí∞ MEDIUM |
| **TF-IDF Optimization** | Easy | 2-3x | Free | ‚úÖ MEDIUM |
| **Model Quantization** | Easy | 2-3x (CPU) | Free | ‚úÖ LOW |
| **Distributed (Ray)** | Hard | 10-100x | $$$ Cluster | üí∞ LOW |

\* Depends on query repetition

### Combined Speedup Estimates

#### **Quick Wins (1-2 days implementation):**
1. Larger batches + FP16: **3-6x**
2. FAISS IVF-PQ: **10-50x** (queries)
3. Parallel UMLS parsing: **10-15x**
4. Streaming: **Memory efficiency**

**Combined Stage 2 Setup: 2-3 hours ‚Üí 15-30 min** (6-12x faster)
**Combined Stage 2 Gen: 15-30 min ‚Üí 2-5 min** (5-10x faster)

#### **Full Optimization (1-2 weeks):**
- All quick wins +
- Multi-GPU (4x) +
- Multi-level caching +
- TF-IDF optimization

**Total Pipeline: 3-5 hours ‚Üí 20-40 min** (5-15x faster)

---

## üõ†Ô∏è 4. IMPLEMENTATION PRIORITY

### Phase 1: Quick Wins (1-2 days) üî•

**Target: 5-10x speedup with minimal code changes**

1. ‚úÖ **Increase batch size + FP16**
   ```python
   # In task_2_1_sapbert_setup.py
   batch_size = 2048  # Was: 256

   # Enable AMP
   from torch.cuda.amp import autocast
   with autocast():
       embeddings = model(**inputs)
   ```

2. ‚úÖ **FAISS IVF-PQ index**
   ```python
   # In task_2_1_sapbert_setup.py
   # Replace IndexFlatIP with IndexIVFPQ
   ```

3. ‚úÖ **Parallel UMLS parsing**
   ```python
   # In umls_loader.py
   # Add multiprocessing to _parse_mrconso()
   ```

4. ‚úÖ **Streaming entity processing**
   ```python
   # In stage2_generate_candidates.py
   # Process entities in batches
   ```

### Phase 2: Infrastructure (3-5 days) ‚öôÔ∏è

**Target: 10-20x speedup with infrastructure changes**

5. **Multi-GPU support**
   - Add DataParallel wrapper
   - Distribute batches across GPUs

6. **Multi-level caching**
   - Implement caching at tokenization, embedding, FAISS levels
   - Add cache invalidation logic

7. **TF-IDF optimization**
   - Chunked processing
   - Sparse matrix storage
   - Float32 instead of float64

### Phase 3: Advanced (1-2 weeks) üöÄ

**Target: 50-100x speedup with distributed computing**

8. **Model quantization**
   - INT8 quantization for CPU deployment

9. **Distributed computing (Ray)**
   - Multi-machine deployment
   - Auto-scaling

---

## üíª 5. CODE EXAMPLES

### Example 1: Optimized SapBERT Encoding

```python
#!/usr/bin/env python3
"""
Optimized SapBERT encoding with:
- Large batches (2048)
- Mixed precision (FP16)
- Multi-GPU support
- Progress estimation
"""

import torch
import torch.nn as nn
from torch.cuda.amp import autocast
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
import numpy as np

def encode_batch_optimized(
    texts,
    model,
    tokenizer,
    device,
    batch_size=2048,
    use_amp=True
):
    """
    Optimized batch encoding with FP16 and large batches

    Speedup: 3-6x vs original implementation
    """
    all_embeddings = []

    for i in tqdm(range(0, len(texts), batch_size), desc="Encoding"):
        batch_texts = texts[i:i+batch_size]

        # Tokenize
        inputs = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=64,
            return_tensors='pt'
        ).to(device)

        # Encode with mixed precision
        with torch.no_grad():
            if use_amp:
                with autocast():
                    outputs = model(**inputs)
                    embeddings = outputs.last_hidden_state[:, 0, :]
            else:
                outputs = model(**inputs)
                embeddings = outputs.last_hidden_state[:, 0, :]

        # Normalize
        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

        # Move to CPU
        all_embeddings.append(embeddings.cpu().numpy())

        # Clear cache
        if device.type == 'cuda':
            torch.cuda.empty_cache()

    return np.vstack(all_embeddings)

# Usage
model = AutoModel.from_pretrained("cambridgeltl/SapBERT-from-PubMedBERT-fulltext")
tokenizer = AutoTokenizer.from_pretrained("cambridgeltl/SapBERT-from-PubMedBERT-fulltext")

# Multi-GPU if available
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
    print(f"Using {torch.cuda.device_count()} GPUs")

model = model.cuda()
model.eval()

# Encode with optimization
embeddings = encode_batch_optimized(
    texts=all_texts,
    model=model,
    tokenizer=tokenizer,
    device=torch.device('cuda'),
    batch_size=2048,  # 8x larger
    use_amp=True      # FP16
)
```

### Example 2: FAISS IVF-PQ Index

```python
#!/usr/bin/env python3
"""
Optimized FAISS indexing with IVF-PQ
Speedup: 10-50x for queries
"""

import faiss
import numpy as np

def build_optimized_faiss_index(
    vectors,
    use_gpu=True,
    nlist=4096,      # Number of IVF clusters
    m=64,            # PQ subvectors
    nbits=8          # Bits per subvector
):
    """
    Build IVF-PQ index for fast approximate search

    Trade-off: 95-99% recall for 10-50x speedup
    """
    dim = vectors.shape[1]

    # Normalize vectors
    faiss.normalize_L2(vectors)

    # Build coarse quantizer
    quantizer = faiss.IndexFlatIP(dim)

    # Build IVF-PQ index
    index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, nbits)

    # GPU support
    if use_gpu and faiss.get_num_gpus() > 0:
        print(f"Using GPU for indexing")
        index = faiss.index_cpu_to_all_gpus(index)

    # Train index (required for IVF)
    print("Training index...")
    index.train(vectors)

    # Add vectors
    print("Adding vectors...")
    index.add(vectors)

    # Set search parameters
    index.nprobe = 32  # Search in 32 clusters (tune for accuracy/speed)

    return index

# Usage
index = build_optimized_faiss_index(
    vectors=umls_embeddings,
    use_gpu=True,
    nlist=4096,
    m=64,
    nbits=8
)

# Search
query_vectors = encode_queries(entities)
faiss.normalize_L2(query_vectors)

k = 128
scores, indices = index.search(query_vectors, k)
```

### Example 3: Parallel UMLS Parsing

```python
#!/usr/bin/env python3
"""
Parallel UMLS parsing with multiprocessing
Speedup: 10-15x with 16 cores
"""

import multiprocessing as mp
from functools import partial
from tqdm import tqdm

def process_mrconso_chunk(chunk_lines, language='ENG'):
    """Process a chunk of MRCONSO lines"""
    concepts = {}

    for line in chunk_lines:
        fields = line.strip().split('|')

        if len(fields) < 15:
            continue

        # Filter by language
        if fields[1] != language:
            continue

        cui = fields[0]
        text = fields[14]
        term_type = fields[12]

        # Initialize concept
        if cui not in concepts:
            concepts[cui] = {
                'cui': cui,
                'preferred_name': None,
                'aliases': set()
            }

        # Store preferred name
        if term_type == 'PT' and not concepts[cui]['preferred_name']:
            concepts[cui]['preferred_name'] = text

        # Store alias
        concepts[cui]['aliases'].add(text)

    return concepts

def parse_mrconso_parallel(file_path, num_processes=16, chunk_size=1_000_000):
    """
    Parse MRCONSO in parallel

    Speedup: ~10-15x with 16 cores
    """
    # Read file in chunks
    print("Reading file...")
    chunks = []

    with open(file_path, 'r', encoding='utf-8') as f:
        chunk = []
        for line in tqdm(f, desc="Loading chunks"):
            chunk.append(line)
            if len(chunk) >= chunk_size:
                chunks.append(chunk)
                chunk = []
        if chunk:
            chunks.append(chunk)

    print(f"Created {len(chunks)} chunks")

    # Parallel processing
    print(f"Processing with {num_processes} processes...")

    with mp.Pool(processes=num_processes) as pool:
        results = list(tqdm(
            pool.imap(process_mrconso_chunk, chunks),
            total=len(chunks),
            desc="Processing"
        ))

    # Merge results
    print("Merging results...")
    final_concepts = {}

    for chunk_result in tqdm(results, desc="Merging"):
        for cui, concept in chunk_result.items():
            if cui not in final_concepts:
                final_concepts[cui] = concept
            else:
                # Merge aliases
                final_concepts[cui]['aliases'].update(concept['aliases'])

                # Update preferred name if missing
                if not final_concepts[cui]['preferred_name'] and concept['preferred_name']:
                    final_concepts[cui]['preferred_name'] = concept['preferred_name']

    return final_concepts

# Usage
concepts = parse_mrconso_parallel(
    file_path='./data/umls/2024AB/META/MRCONSO.RRF',
    num_processes=16,
    chunk_size=1_000_000
)
```

---

## üìä 6. BENCHMARKING

### Benchmark Setup

```python
import time
from contextlib import contextmanager

@contextmanager
def timer(name):
    """Context manager for timing"""
    start = time.time()
    yield
    end = time.time()
    print(f"{name}: {end - start:.2f} seconds")

# Benchmark encoding
with timer("Baseline (batch=256, FP32)"):
    embeddings_baseline = encode_batch(texts, batch_size=256, use_amp=False)

with timer("Optimized (batch=2048, FP16)"):
    embeddings_optimized = encode_batch(texts, batch_size=2048, use_amp=True)

# Benchmark FAISS
with timer("Exact search (IndexFlatIP)"):
    scores_exact, indices_exact = index_exact.search(queries, k=128)

with timer("Approximate search (IndexIVFPQ)"):
    scores_approx, indices_approx = index_ivfpq.search(queries, k=128)

# Measure recall
recall = compute_recall(indices_exact, indices_approx, k=128)
print(f"Recall@128: {recall:.3f}")
```

### Expected Benchmark Results

```
BASELINE (Current Implementation):
- SapBERT encoding (4M concepts):    2.5 hours (GPU)
- FAISS index build:                 15 minutes
- FAISS search (10K queries):        45 seconds
- UMLS parse:                        35 minutes
- Total first run:                   3.5 hours

OPTIMIZED (Phase 1):
- SapBERT encoding (batch=2048, FP16): 25 minutes (10x faster)
- FAISS index build (IVF-PQ):          3 minutes (5x faster)
- FAISS search (IVF-PQ):               2 seconds (22x faster)
- UMLS parse (parallel):               2 minutes (17x faster)
- Total first run:                     35 minutes (6x faster)

OPTIMIZED (Phase 2 - Multi-GPU):
- SapBERT encoding (4x GPU):         6 minutes (25x faster)
- Total first run:                   15 minutes (14x faster)
```

---

## üéØ 7. RECOMMENDED IMPLEMENTATION PLAN

### Week 1: Quick Wins
- **Day 1-2:** Implement larger batches + FP16 encoding
- **Day 3:** Implement FAISS IVF-PQ indexing
- **Day 4:** Implement parallel UMLS parsing
- **Day 5:** Testing and benchmarking

**Expected Result:** 5-10x speedup, pipeline 3-5 hours ‚Üí 30-60 min

### Week 2: Infrastructure
- **Day 1-2:** Implement streaming processing
- **Day 3-4:** Implement multi-level caching
- **Day 5:** TF-IDF optimization

**Expected Result:** Additional 2-3x speedup, better memory efficiency

### Week 3+: Advanced (Optional)
- Multi-GPU support
- Distributed computing with Ray
- Model quantization

**Expected Result:** 20-50x total speedup for very large datasets

---

## ‚úÖ 8. CONCLUSION

### Summary of Recommendations

**üî• Highest Priority (Implement First):**
1. ‚úÖ Larger batches (2048) + Mixed precision (FP16) ‚Üí **3-6x faster**
2. ‚úÖ FAISS IVF-PQ indexing ‚Üí **10-50x faster queries**
3. ‚úÖ Parallel UMLS parsing ‚Üí **10-15x faster**
4. ‚úÖ Streaming entity processing ‚Üí **Memory efficiency**

**‚öôÔ∏è Medium Priority:**
5. Multi-level caching ‚Üí **2-10x faster** (depends on repetition)
6. TF-IDF optimization ‚Üí **2-3x faster**
7. Multi-GPU support ‚Üí **4-8x faster** (if GPUs available)

**üöÄ Advanced (Large Scale):**
8. Distributed computing (Ray) ‚Üí **10-100x** scalability
9. Model quantization ‚Üí **2-3x faster CPU**

### Expected Outcomes

| Metric | Before | After (Quick Wins) | After (Full) |
|--------|--------|-------------------|--------------|
| **First run** | 3-5 hours | 30-60 min | 15-30 min |
| **Cached run** | 30-60 min | 5-10 min | 2-5 min |
| **Memory usage** | ~30 GB | ~15 GB | ~10 GB |
| **Scalability** | Linear | Sub-linear | Distributed |
| **GPU utilization** | 30-50% | 80-95% | 95-100% |

### ROI Analysis

**Implementation Time vs Speedup:**
- Quick wins (5 days): **5-10x speedup** ‚Üí Best ROI
- Infrastructure (10 days): **10-20x speedup** ‚Üí Good ROI
- Advanced (20+ days): **20-50x speedup** ‚Üí High cost

**Recommendation:** Start with **Quick Wins (Phase 1)** for best cost/benefit ratio.

---

**T√†i li·ªáu n√†y cung c·∫•p roadmap ho√†n ch·ªânh ƒë·ªÉ optimize UMLS mapping pipeline t·ª´ 3-5 gi·ªù xu·ªëng 15-30 ph√∫t!**
